{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-xDfYQqevGU"
      },
      "source": [
        "## Machine translation from english to arabic Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY8QinO9eeCS"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9qWJ_u77Mng",
        "outputId": "fda867ea-80ce-4915-d53d-eaf145f5b152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: transformers[sentencepiece]\n"
          ]
        }
      ],
      "source": [
        "# prompt: install dependencies for datasets to use load_dataset\n",
        "\n",
        "# ## Machine translation from english to arabic Project\n",
        "!pip install datasets transformers[sentencepiece] sacrebleu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "43OgwbxGeed1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsV0n3omfEYh"
      },
      "source": [
        "## Search for Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIoubJB0fEq9",
        "outputId": "aeda5fa6-bd4f-4427-f0f1-3b2d20b090df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db241946411e4429a248e890bdf1561e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddbf6f629261483cb36b0c468e34e8b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/9.83M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5ff4a833d9344b9a3073a492f740d30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/51974 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Loading the parallel-sentences-global-voices dataset from Hugging Face\n",
        "dataset = load_dataset(\"sentence-transformers/parallel-sentences-global-voices\", name=\"en-ar\", split=\"train\", trust_remote_code=True)\n",
        "\n",
        "# Extract English and Arabic sentences\n",
        "data = {\n",
        "    'english': [example['english'] for example in dataset],\n",
        "    'arabic': [example['non_english'] for example in dataset]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "data = df[['english', 'arabic']].dropna()\n",
        "data['english'] = data['english'].astype(str).str.strip()\n",
        "data['arabic'] = data['arabic'].astype(str).str.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH82121Nfy2C"
      },
      "source": [
        "## Perform Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrXECpqogBaS"
      },
      "source": [
        "Tokenizing and padding the sentences, adding start/end tokens for Arabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3LuphQ_Tj-mT"
      },
      "outputs": [],
      "source": [
        "# Add start and end tokens to Arabic sentences\n",
        "data['arabic'] = data['arabic'].apply(lambda x: '<start> ' + x + ' <end>')\n",
        "\n",
        "# Limit to 10000 samples for computational efficiency, or use all if fewer\n",
        "data = data.sample(n=min(10000, len(data)), random_state=42).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tCNLBmasfzNW"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_pad(texts, max_len=None):\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    if max_len is None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "    return padded, tokenizer, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fjqFse8FgGIC"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad English and Arabic sentences\n",
        "eng_padded, eng_tokenizer, eng_max_len = tokenize_and_pad(data['english'])\n",
        "ar_padded, ar_tokenizer, ar_max_len = tokenize_and_pad(data['arabic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kv07NPJDgGRr"
      },
      "outputs": [],
      "source": [
        "# Split data into train, validation, and test sets\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    eng_padded, ar_padded, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa7nllCPgTsN"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J50-X16gb2-"
      },
      "source": [
        "Defining custom attention mechanisms and Transformer components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tAJfTg1dDp9T"
      },
      "outputs": [],
      "source": [
        "class CustomMultiHeadAttention(Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(CustomMultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.depth = embed_dim // num_heads\n",
        "\n",
        "        self.wq = Dense(embed_dim)\n",
        "        self.wk = Dense(embed_dim)\n",
        "        self.wv = Dense(embed_dim)\n",
        "        self.dense = Dense(embed_dim)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, query, value, key=None, training=None):\n",
        "        if key is None:\n",
        "            key = value\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        q = self.wq(query)  # (batch_size, seq_len, embed_dim)\n",
        "        k = self.wk(key)    # (batch_size, seq_len, embed_dim)\n",
        "        v = self.wv(value)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "        scaled_attention = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
        "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
        "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
        "        output = tf.reshape(output, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.dense(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-Z7snOEwgT7Y"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query: (batch_size, dec_seq_len, embed_dim)\n",
        "        # values: (batch_size, enc_seq_len, embed_dim)\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        query_exp = tf.expand_dims(query, 2)  # (batch_size, dec_seq_len, 1, embed_dim)\n",
        "        values_exp = tf.expand_dims(values, 1)  # (batch_size, 1, enc_seq_len, embed_dim)\n",
        "\n",
        "        # Compute score\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_exp) + self.W2(values_exp)))  # (batch_size, dec_seq_len, enc_seq_len, 1)\n",
        "        score = tf.squeeze(score, axis=-1)  # (batch_size, dec_seq_len, enc_seq_len)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = tf.nn.softmax(score, axis=-1)  # (batch_size, dec_seq_len, enc_seq_len)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_vector = tf.matmul(attention_weights, values)  # (batch_size, dec_seq_len, embed_dim)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q9KrMXEEgl3E"
      },
      "outputs": [],
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, attention_type='multihead', rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.attention_type = attention_type\n",
        "        if attention_type == 'multihead':\n",
        "            self.attention = CustomMultiHeadAttention(embed_dim, num_heads)\n",
        "        else:\n",
        "            self.attention = AdditiveAttention(embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        emb = self.embedding(inputs)  # (batch_size, enc_seq_len, embed_dim)\n",
        "        if self.attention_type == 'multihead':\n",
        "            attn_output = self.attention(emb, emb)  # (batch_size, enc_seq_len, embed_dim)\n",
        "        else:\n",
        "            attn_output, _ = self.attention(emb, emb)  # (batch_size, enc_seq_len, embed_dim)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(emb + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OhTwQ6CZgrKL"
      },
      "outputs": [],
      "source": [
        "class Decoder(Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, attention_type='multihead', rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.attention_type = attention_type\n",
        "        if attention_type == 'multihead':\n",
        "            self.self_attention = CustomMultiHeadAttention(embed_dim, num_heads)\n",
        "            self.enc_attention = CustomMultiHeadAttention(embed_dim, num_heads)\n",
        "        else:\n",
        "            self.self_attention = AdditiveAttention(embed_dim)\n",
        "            self.enc_attention = AdditiveAttention(embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        emb = self.embedding(inputs)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        if self.attention_type == 'multihead':\n",
        "            self_attn_output = self.self_attention(emb, emb)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        else:\n",
        "            self_attn_output, _ = self.self_attention(emb, emb)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
        "        out1 = self.layernorm1(emb + self_attn_output)\n",
        "\n",
        "        if self.attention_type == 'multihead':\n",
        "            enc_attn_output = self.enc_attention(out1, enc_output)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        else:\n",
        "            enc_attn_output, _ = self.enc_attention(out1, enc_output)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        enc_attn_output = self.dropout2(enc_attn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + enc_attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pWUANI8EkxPY"
      },
      "outputs": [],
      "source": [
        "def build_transformer_model(vocab_size_enc, vocab_size_dec, max_len_enc, max_len_dec, attention_type='multihead', embed_dim=256, num_heads=8, ff_dim=512, training=None):\n",
        "    encoder_inputs = Input(shape=(max_len_enc,))\n",
        "    decoder_inputs = Input(shape=(max_len_dec,))\n",
        "\n",
        "    encoder = Encoder(vocab_size_enc, embed_dim, num_heads, ff_dim, attention_type)\n",
        "    enc_output = encoder(encoder_inputs, training=training)\n",
        "\n",
        "    decoder = Decoder(vocab_size_dec, embed_dim, num_heads, ff_dim, attention_type)\n",
        "    dec_output = decoder(decoder_inputs, enc_output, training=training)\n",
        "\n",
        "    outputs = Dense(vocab_size_dec, activation='softmax')(dec_output)\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbhBdUvxgwoO"
      },
      "source": [
        "## Tune Parameters and Compare Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sEhdIE2g7S1"
      },
      "source": [
        "Training models with MultiHead and Additive attention, using early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y9DTWxgdgxAB"
      },
      "outputs": [],
      "source": [
        "attention_types = ['multihead', 'additive']\n",
        "bleu_scores = {}\n",
        "models = {}\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oyggxkCmFY9b"
      },
      "outputs": [],
      "source": [
        "# Callback to evaluate and print test loss and accuracy per epoch\n",
        "test_callback = LambdaCallback(\n",
        "    on_epoch_end=lambda epoch, logs: print(f\"\\nTest Loss: {model.evaluate([X_test, y_test[:, :-1]], y_test[:, 1:, np.newaxis], verbose=0)[0]:.4f}, \"\n",
        "                                          f\"Test Accuracy: {model.evaluate([X_test, y_test[:, :-1]], y_test[:, 1:, np.newaxis], verbose=0)[1]:.4f}\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_RlZPaulTae",
        "outputId": "d07c580a-e5d7-4e86-c8bd-a043cd039931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training model with multihead attention\n",
            "Epoch 1/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - accuracy: 0.8740 - loss: 4.3421 \n",
            "Test Loss: 0.7056, Test Accuracy: 0.9266\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2103s\u001b[0m 21s/step - accuracy: 0.8744 - loss: 4.3192 - val_accuracy: 0.9264 - val_loss: 0.7100\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.9254 - loss: 0.7011 \n",
            "Test Loss: 0.7096, Test Accuracy: 0.9266\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1734s\u001b[0m 17s/step - accuracy: 0.9254 - loss: 0.7011 - val_accuracy: 0.9265 - val_loss: 0.7143\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41s/step - accuracy: 0.9253 - loss: 0.6879 \n",
            "Test Loss: 0.7153, Test Accuracy: 0.9265\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4470s\u001b[0m 45s/step - accuracy: 0.9253 - loss: 0.6878 - val_accuracy: 0.9261 - val_loss: 0.7199\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33s/step - accuracy: 0.9267 - loss: 0.6666 \n",
            "Test Loss: 0.7258, Test Accuracy: 0.9266\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4296s\u001b[0m 43s/step - accuracy: 0.9267 - loss: 0.6666 - val_accuracy: 0.9263 - val_loss: 0.7307\n",
            "\n",
            "Training model with additive attention\n",
            "Epoch 1/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483s/step - accuracy: 0.8744 - loss: 4.2205  \n",
            "Test Loss: 0.6994, Test Accuracy: 0.9266\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53846s\u001b[0m 543s/step - accuracy: 0.8748 - loss: 4.1981 - val_accuracy: 0.9264 - val_loss: 0.7035\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46s/step - accuracy: 0.9265 - loss: 0.6717 \n",
            "Test Loss: 0.6761, Test Accuracy: 0.9298\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4865s\u001b[0m 49s/step - accuracy: 0.9265 - loss: 0.6717 - val_accuracy: 0.9295 - val_loss: 0.6800\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31s/step - accuracy: 0.9313 - loss: 0.5818 \n",
            "Test Loss: 0.6672, Test Accuracy: 0.9306\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3534s\u001b[0m 36s/step - accuracy: 0.9313 - loss: 0.5817 - val_accuracy: 0.9301 - val_loss: 0.6711\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32s/step - accuracy: 0.9402 - loss: 0.4680 \n",
            "Test Loss: 0.6719, Test Accuracy: 0.9305\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5254s\u001b[0m 53s/step - accuracy: 0.9402 - loss: 0.4680 - val_accuracy: 0.9301 - val_loss: 0.6761\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39s/step - accuracy: 0.9517 - loss: 0.3649 \n",
            "Test Loss: 0.6821, Test Accuracy: 0.9304\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4350s\u001b[0m 44s/step - accuracy: 0.9517 - loss: 0.3649 - val_accuracy: 0.9300 - val_loss: 0.6863\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43s/step - accuracy: 0.9535 - loss: 0.2873 \n",
            "Test Loss: 0.6982, Test Accuracy: 0.9303\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4634s\u001b[0m 47s/step - accuracy: 0.9535 - loss: 0.2873 - val_accuracy: 0.9300 - val_loss: 0.7028\n"
          ]
        }
      ],
      "source": [
        "for attention_type in attention_types:\n",
        "    print(f\"\\nTraining model with {attention_type} attention\")\n",
        "    model = build_transformer_model(\n",
        "        vocab_size_enc=len(eng_tokenizer.word_index) + 1,\n",
        "        vocab_size_dec=len(ar_tokenizer.word_index) + 1,\n",
        "        max_len_enc=eng_max_len,\n",
        "        max_len_dec=ar_max_len,\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Pad the decoder inputs to match max_len_dec\n",
        "    dec_input_train = pad_sequences(y_train[:, :-1], maxlen=ar_max_len, padding='post')\n",
        "    dec_input_val = pad_sequences(y_val[:, :-1], maxlen=ar_max_len, padding='post')\n",
        "\n",
        "    # Pad the target data to match max_len_dec\n",
        "    target_train = pad_sequences(y_train[:, 1:], maxlen=ar_max_len, padding='post', value=0)\n",
        "    target_val = pad_sequences(y_val[:, 1:], maxlen=ar_max_len, padding='post', value=0)\n",
        "\n",
        "    # Train the model with callbacks for test evaluation\n",
        "    history = model.fit(\n",
        "        [X_train, dec_input_train], target_train[:, :, np.newaxis],\n",
        "        validation_data=([X_val, dec_input_val], target_val[:, :, np.newaxis]),\n",
        "        epochs=10,\n",
        "        batch_size=64,\n",
        "        callbacks=[early_stopping, test_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "    models[attention_type] = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pgEDSaDhEZV"
      },
      "source": [
        "## Evaluate The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23G1majzhEhL"
      },
      "source": [
        "Evaluating both models using BLEU score on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hN1P38EKhEzS"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, eng_tokenizer, ar_tokenizer, eng_max_len, ar_max_len):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    enc_input = pad_sequences(seq, maxlen=eng_max_len, padding='post')\n",
        "\n",
        "    dec_input = np.zeros((1, ar_max_len))\n",
        "    dec_input[0, 0] = ar_tokenizer.word_index['<start>']\n",
        "\n",
        "    for i in range(1, ar_max_len):\n",
        "        pred = model.predict([enc_input, dec_input], verbose=0)\n",
        "        next_token = np.argmax(pred[0, i-1, :])\n",
        "        dec_input[0, i] = next_token\n",
        "        if next_token == ar_tokenizer.word_index['<end>']:\n",
        "            break\n",
        "\n",
        "    ar_words = []\n",
        "    for idx in dec_input[0]:\n",
        "        if idx == 0 or idx == ar_tokenizer.word_index['<start>'] or idx == ar_tokenizer.word_index['<end>']:\n",
        "            continue\n",
        "        word = ar_tokenizer.index_word.get(idx, '')\n",
        "        if word:\n",
        "            ar_words.append(word)\n",
        "    return ' '.join(ar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eDNCdYU1hQA6",
        "outputId": "38852429-62ec-4a92-a9f3-33d2545cb136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score on Test Set (multihead attention): 0.0000\n",
            "BLEU Score on Test Set (additive attention): 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Calculate BLEU scores for both models\n",
        "for attention_type in attention_types:\n",
        "    model = models[attention_type]\n",
        "    references = [[ar_tokenizer.sequences_to_texts([y_test[i]])[0].replace('<start>', '').replace('<end>', '').strip().split()] for i in range(len(y_test))]\n",
        "    candidates = []\n",
        "    for i in range(len(X_test)):\n",
        "        eng_sentence = eng_tokenizer.sequences_to_texts([X_test[i]])[0]\n",
        "        pred_sentence = translate_sentence(model, eng_sentence, eng_tokenizer, ar_tokenizer, eng_max_len, ar_max_len)\n",
        "        candidates.append(pred_sentence.split())\n",
        "\n",
        "    bleu_score = corpus_bleu(references, candidates)\n",
        "    bleu_scores[attention_type] = bleu_score\n",
        "    print(f'BLEU Score on Test Set ({attention_type} attention): {bleu_score:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02LV8PlJlvoe",
        "outputId": "51794752-1537-4bb1-a3b6-7c8b61af160a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attention Mechanism Comparison:\n",
            "Multihead Attention BLEU Score: 0.0000\n",
            "Additive Attention BLEU Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Compare attention mechanisms\n",
        "print(\"\\nAttention Mechanism Comparison:\")\n",
        "for attention_type, score in bleu_scores.items():\n",
        "    print(f\"{attention_type.capitalize()} Attention BLEU Score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e8IFgAphbLa"
      },
      "source": [
        "## Add Test Data with Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BAAbaPDhbTE"
      },
      "source": [
        "Testing with sample sentences using the better-performing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fUf8ekytl9MW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using multihead attention model for test translations\n"
          ]
        }
      ],
      "source": [
        "best_attention = max(bleu_scores, key=bleu_scores.get)\n",
        "best_model = models[best_attention]\n",
        "print(f\"\\nUsing {best_attention} attention model for test translations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dh5qAEEQhbgY"
      },
      "outputs": [],
      "source": [
        "test_sentences = [\n",
        "    \"I love to read books.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Can you help me?\",\n",
        "    \"This is a beautiful house.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ty3LFx0ZhbnX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Translations:\n",
            "English: I love to read books.\n",
            "Arabic: \n",
            "\n",
            "English: The weather is nice today.\n",
            "Arabic: \n",
            "\n",
            "English: Can you help me?\n",
            "Arabic: \n",
            "\n",
            "English: This is a beautiful house.\n",
            "Arabic: \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTest Translations:\")\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_sentence(best_model, sentence, eng_tokenizer, ar_tokenizer, eng_max_len, ar_max_len)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Arabic: {translation}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
