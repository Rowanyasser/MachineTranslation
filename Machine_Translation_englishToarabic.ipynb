{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Machine translation from english to arabic Project"
      ],
      "metadata": {
        "id": "U-xDfYQqevGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "yY8QinO9eeCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: install dependencies for datasets to use load_dataset\n",
        "\n",
        "# ## Machine translation from english to arabic Project\n",
        "!pip install datasets transformers[sentencepiece] sacrebleu\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9qWJ_u77Mng",
        "outputId": "fda867ea-80ce-4915-d53d-eaf145f5b152"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "43OgwbxGeed1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search for Data"
      ],
      "metadata": {
        "id": "nsV0n3omfEYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the parallel-sentences-global-voices dataset from Hugging Face\n",
        "dataset = load_dataset(\"sentence-transformers/parallel-sentences-global-voices\", name=\"en-ar\", split=\"train\", trust_remote_code=True)\n",
        "\n",
        "# Extract English and Arabic sentences\n",
        "data = {\n",
        "    'english': [example['english'] for example in dataset],\n",
        "    'arabic': [example['non_english'] for example in dataset]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "data = df[['english', 'arabic']].dropna()\n",
        "data['english'] = data['english'].astype(str).str.strip()\n",
        "data['arabic'] = data['arabic'].astype(str).str.strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIoubJB0fEq9",
        "outputId": "aeda5fa6-bd4f-4427-f0f1-3b2d20b090df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Preprocessing"
      ],
      "metadata": {
        "id": "qH82121Nfy2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing and padding the sentences, adding start/end tokens for Arabic"
      ],
      "metadata": {
        "id": "lrXECpqogBaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add start and end tokens to Arabic sentences\n",
        "data['arabic'] = data['arabic'].apply(lambda x: '<start> ' + x + ' <end>')\n",
        "\n",
        "# Limit to 10000 samples for computational efficiency, or use all if fewer\n",
        "data = data.sample(n=min(10000, len(data)), random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "3LuphQ_Tj-mT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad(texts, max_len=None):\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    if max_len is None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "    return padded, tokenizer, max_len"
      ],
      "metadata": {
        "id": "tCNLBmasfzNW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad English and Arabic sentences\n",
        "eng_padded, eng_tokenizer, eng_max_len = tokenize_and_pad(data['english'])\n",
        "ar_padded, ar_tokenizer, ar_max_len = tokenize_and_pad(data['arabic'])"
      ],
      "metadata": {
        "id": "fjqFse8FgGIC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train, validation, and test sets\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    eng_padded, ar_padded, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Kv07NPJDgGRr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build The Model"
      ],
      "metadata": {
        "id": "Xa7nllCPgTsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining custom attention mechanisms and Transformer components"
      ],
      "metadata": {
        "id": "3J50-X16gb2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMultiHeadAttention(Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(CustomMultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.depth = embed_dim // num_heads\n",
        "\n",
        "        self.wq = Dense(embed_dim)\n",
        "        self.wk = Dense(embed_dim)\n",
        "        self.wv = Dense(embed_dim)\n",
        "        self.dense = Dense(embed_dim)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, query, value, key=None, training=None):\n",
        "        if key is None:\n",
        "            key = value\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        q = self.wq(query)  # (batch_size, seq_len, embed_dim)\n",
        "        k = self.wk(key)    # (batch_size, seq_len, embed_dim)\n",
        "        v = self.wv(value)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "        scaled_attention = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
        "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
        "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
        "        output = tf.reshape(output, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.dense(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "tAJfTg1dDp9T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query: (batch_size, dec_seq_len, embed_dim)\n",
        "        # values: (batch_size, enc_seq_len, embed_dim)\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        query_exp = tf.expand_dims(query, 2)  # (batch_size, dec_seq_len, 1, embed_dim)\n",
        "        values_exp = tf.expand_dims(values, 1)  # (batch_size, 1, enc_seq_len, embed_dim)\n",
        "\n",
        "        # Compute score\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_exp) + self.W2(values_exp)))  # (batch_size, dec_seq_len, enc_seq_len, 1)\n",
        "        score = tf.squeeze(score, axis=-1)  # (batch_size, dec_seq_len, enc_seq_len)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = tf.nn.softmax(score, axis=-1)  # (batch_size, dec_seq_len, enc_seq_len)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_vector = tf.matmul(attention_weights, values)  # (batch_size, dec_seq_len, embed_dim)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "-Z7snOEwgT7Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, attention_type='multihead', rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.attention_type = attention_type\n",
        "        if attention_type == 'multihead':\n",
        "            self.attention = CustomMultiHeadAttention(embed_dim, num_heads)\n",
        "        else:\n",
        "            self.attention = AdditiveAttention(embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        emb = self.embedding(inputs)  # (batch_size, enc_seq_len, embed_dim)\n",
        "        if self.attention_type == 'multihead':\n",
        "            attn_output = self.attention(emb, emb)  # (batch_size, enc_seq_len, embed_dim)\n",
        "        else:\n",
        "            attn_output, _ = self.attention(emb, emb)  # (batch_size, enc_seq_len, embed_dim)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(emb + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "q9KrMXEEgl3E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, attention_type='multihead', rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.attention_type = attention_type\n",
        "        if attention_type == 'multihead':\n",
        "            self.self_attention = CustomMultiHeadAttention(embed_dim, num_heads)\n",
        "            self.enc_attention = CustomMultiHeadAttention(embed_dim, num_heads)\n",
        "        else:\n",
        "            self.self_attention = AdditiveAttention(embed_dim)\n",
        "            self.enc_attention = AdditiveAttention(embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        emb = self.embedding(inputs)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        if self.attention_type == 'multihead':\n",
        "            self_attn_output = self.self_attention(emb, emb)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        else:\n",
        "            self_attn_output, _ = self.self_attention(emb, emb)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
        "        out1 = self.layernorm1(emb + self_attn_output)\n",
        "\n",
        "        if self.attention_type == 'multihead':\n",
        "            enc_attn_output = self.enc_attention(out1, enc_output)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        else:\n",
        "            enc_attn_output, _ = self.enc_attention(out1, enc_output)  # (batch_size, dec_seq_len, embed_dim)\n",
        "        enc_attn_output = self.dropout2(enc_attn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + enc_attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)"
      ],
      "metadata": {
        "id": "OhTwQ6CZgrKL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer_model(vocab_size_enc, vocab_size_dec, max_len_enc, max_len_dec, attention_type='multihead', embed_dim=256, num_heads=8, ff_dim=512, training=None):\n",
        "    encoder_inputs = Input(shape=(max_len_enc,))\n",
        "    decoder_inputs = Input(shape=(max_len_dec,))\n",
        "\n",
        "    encoder = Encoder(vocab_size_enc, embed_dim, num_heads, ff_dim, attention_type)\n",
        "    enc_output = encoder(encoder_inputs, training=training)\n",
        "\n",
        "    decoder = Decoder(vocab_size_dec, embed_dim, num_heads, ff_dim, attention_type)\n",
        "    dec_output = decoder(decoder_inputs, enc_output, training=training)\n",
        "\n",
        "    outputs = Dense(vocab_size_dec, activation='softmax')(dec_output)\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "pWUANI8EkxPY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune Parameters and Compare Attention Mechanisms"
      ],
      "metadata": {
        "id": "dbhBdUvxgwoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training models with MultiHead and Additive attention, using early stopping"
      ],
      "metadata": {
        "id": "1sEhdIE2g7S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_types = ['multihead', 'additive']\n",
        "bleu_scores = {}\n",
        "models = {}\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "Y9DTWxgdgxAB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback to evaluate and print test loss and accuracy per epoch\n",
        "test_callback = LambdaCallback(\n",
        "    on_epoch_end=lambda epoch, logs: print(f\"\\nTest Loss: {model.evaluate([X_test, y_test[:, :-1]], y_test[:, 1:, np.newaxis], verbose=0)[0]:.4f}, \"\n",
        "                                          f\"Test Accuracy: {model.evaluate([X_test, y_test[:, :-1]], y_test[:, 1:, np.newaxis], verbose=0)[1]:.4f}\")\n",
        ")"
      ],
      "metadata": {
        "id": "oyggxkCmFY9b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for attention_type in attention_types:\n",
        "    print(f\"\\nTraining model with {attention_type} attention\")\n",
        "    model = build_transformer_model(\n",
        "        vocab_size_enc=len(eng_tokenizer.word_index) + 1,\n",
        "        vocab_size_dec=len(ar_tokenizer.word_index) + 1,\n",
        "        max_len_enc=eng_max_len,\n",
        "        max_len_dec=ar_max_len,\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Pad the decoder inputs to match max_len_dec\n",
        "    dec_input_train = pad_sequences(y_train[:, :-1], maxlen=ar_max_len, padding='post')\n",
        "    dec_input_val = pad_sequences(y_val[:, :-1], maxlen=ar_max_len, padding='post')\n",
        "\n",
        "    # Pad the target data to match max_len_dec\n",
        "    target_train = pad_sequences(y_train[:, 1:], maxlen=ar_max_len, padding='post', value=0)\n",
        "    target_val = pad_sequences(y_val[:, 1:], maxlen=ar_max_len, padding='post', value=0)\n",
        "\n",
        "    # Train the model with callbacks for test evaluation\n",
        "    history = model.fit(\n",
        "        [X_train, dec_input_train], target_train[:, :, np.newaxis],\n",
        "        validation_data=([X_val, dec_input_val], target_val[:, :, np.newaxis]),\n",
        "        epochs=10,\n",
        "        batch_size=64,\n",
        "        callbacks=[early_stopping, test_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "    models[attention_type] = model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_RlZPaulTae",
        "outputId": "d07c580a-e5d7-4e86-c8bd-a043cd039931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with multihead attention\n",
            "Epoch 1/20\n",
            "\u001b[1m  9/100\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30:37\u001b[0m 60s/step - accuracy: 0.6329 - loss: 9.6615"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate The Model"
      ],
      "metadata": {
        "id": "1pgEDSaDhEZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating both models using BLEU score on test data"
      ],
      "metadata": {
        "id": "23G1majzhEhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, eng_tokenizer, ar_tokenizer, eng_max_len, ar_max_len):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    enc_input = pad_sequences(seq, maxlen=eng_max_len, padding='post')\n",
        "\n",
        "    dec_input = np.zeros((1, ar_max_len))\n",
        "    dec_input[0, 0] = ar_tokenizer.word_index['<start>']\n",
        "\n",
        "    for i in range(1, ar_max_len):\n",
        "        pred = model.predict([enc_input, dec_input], verbose=0)\n",
        "        next_token = np.argmax(pred[0, i-1, :])\n",
        "        dec_input[0, i] = next_token\n",
        "        if next_token == ar_tokenizer.word_index['<end>']:\n",
        "            break\n",
        "\n",
        "    ar_words = []\n",
        "    for idx in dec_input[0]:\n",
        "        if idx == 0 or idx == ar_tokenizer.word_index['<start>'] or idx == ar_tokenizer.word_index['<end>']:\n",
        "            continue\n",
        "        word = ar_tokenizer.index_word.get(idx, '')\n",
        "        if word:\n",
        "            ar_words.append(word)\n",
        "    return ' '.join(ar_words)"
      ],
      "metadata": {
        "id": "hN1P38EKhEzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate BLEU scores for both models\n",
        "for attention_type in attention_types:\n",
        "    model = models[attention_type]\n",
        "    references = [[ar_tokenizer.sequences_to_texts([y_test[i]])[0].replace('<start>', '').replace('<end>', '').strip().split()] for i in range(len(y_test))]\n",
        "    candidates = []\n",
        "    for i in range(len(X_test)):\n",
        "        eng_sentence = eng_tokenizer.sequences_to_texts([X_test[i]])[0]\n",
        "        pred_sentence = translate_sentence(model, eng_sentence, eng_tokenizer, ar_tokenizer, eng_max_len, ar_max_len)\n",
        "        candidates.append(pred_sentence.split())\n",
        "\n",
        "    bleu_score = corpus_bleu(references, candidates)\n",
        "    bleu_scores[attention_type] = bleu_score\n",
        "    print(f'BLEU Score on Test Set ({attention_type} attention): {bleu_score:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eDNCdYU1hQA6",
        "outputId": "38852429-62ec-4a92-a9f3-33d2545cb136"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'multihead'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0f906fe49a5f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate BLEU scores for both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattention_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattention_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mar_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<start>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'multihead'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare attention mechanisms\n",
        "print(\"\\nAttention Mechanism Comparison:\")\n",
        "for attention_type, score in bleu_scores.items():\n",
        "    print(f\"{attention_type.capitalize()} Attention BLEU Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02LV8PlJlvoe",
        "outputId": "51794752-1537-4bb1-a3b6-7c8b61af160a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attention Mechanism Comparison:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Test Data with Notebook"
      ],
      "metadata": {
        "id": "2e8IFgAphbLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with sample sentences using the better-performing model"
      ],
      "metadata": {
        "id": "0BAAbaPDhbTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_attention = max(bleu_scores, key=bleu_scores.get)\n",
        "best_model = models[best_attention]\n",
        "print(f\"\\nUsing {best_attention} attention model for test translations\")"
      ],
      "metadata": {
        "id": "fUf8ekytl9MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"I love to read books.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Can you help me?\",\n",
        "    \"This is a beautiful house.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "dh5qAEEQhbgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTest Translations:\")\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_sentence(best_model, sentence, eng_tokenizer, ar_tokenizer, eng_max_len, ar_max_len)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Arabic: {translation}\\n\")"
      ],
      "metadata": {
        "id": "ty3LFx0ZhbnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}